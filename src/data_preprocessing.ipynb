{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32e594f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns for SPY: ['Date', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n",
      "Columns for TSLA: ['Date', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n",
      "                 Close        High         Low        Open     Volume\n",
      "Date                                                                 \n",
      "2015-07-01  174.917114  175.363889  174.124717  175.110995  135979900\n",
      "2015-07-02  174.756927  175.566188  174.335441  175.397596  104373700\n",
      "2015-07-06  174.259628  175.043588  173.256487  173.458805  117975400\n",
      "2015-07-07  175.355438  175.481879  172.059407  174.461888  173820200\n",
      "2015-07-08  172.413498  174.293327  172.177466  174.006719  164020100\n",
      "Saved SPY data to processed_data\\SPY_processed.csv\n",
      "Saved TSLA data to processed_data\\TSLA_processed.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module: data_preprocessing.py\n",
    "Purpose: Clean and prepare raw financial data for modeling.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def preprocess_data(data_dict, save_path=\"data/processed\"):\n",
    "    \"\"\"\n",
    "    Preprocess raw data: clean, merge, and feature engineer.\n",
    "\n",
    "    Args:\n",
    "        data_dict (dict): Dictionary of DataFrames {ticker: DataFrame}\n",
    "        save_path (str): Directory to save processed data\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed DataFrames for each ticker\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    processed_dict = {}\n",
    "\n",
    "    for ticker, df in data_dict.items():\n",
    "        print(f\"ðŸ›  Preprocessing {ticker}...\")\n",
    "\n",
    "        # Ensure datetime index\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Sort by date\n",
    "        df.sort_index(inplace=True)\n",
    "\n",
    "        # Handle missing values (forward fill, then backward fill)\n",
    "        df.ffill(inplace=True)\n",
    "        df.bfill(inplace=True)\n",
    "\n",
    "        # Feature Engineering\n",
    "        df['Daily_Return'] = df['Adj Close'].pct_change()\n",
    "        df['Rolling_Mean_20'] = df['Adj Close'].rolling(window=20).mean()\n",
    "        df['Rolling_Std_20'] = df['Adj Close'].rolling(window=20).std()\n",
    "\n",
    "        # Stationarity test (ADF on returns)\n",
    "        adf_pvalue = np.nan\n",
    "        try:\n",
    "            result = adfuller(df['Daily_Return'].dropna())\n",
    "            adf_pvalue = result[1]\n",
    "        except Exception:\n",
    "            pass\n",
    "        df.attrs['ADF_pvalue'] = adf_pvalue\n",
    "        print(f\"   ðŸ“Š ADF p-value (returns): {adf_pvalue:.4f}\")\n",
    "\n",
    "        # Save processed CSV\n",
    "        file_path = os.path.join(save_path, f\"{ticker}_processed.csv\")\n",
    "        df.to_csv(file_path)\n",
    "        print(f\"   âœ… Saved processed data to {file_path}\")\n",
    "\n",
    "        processed_dict[ticker] = df\n",
    "\n",
    "    return processed_dict\n",
    "\n",
    "\n",
    "def merge_assets(processed_dict):\n",
    "    \"\"\"\n",
    "    Merge multiple assets' adjusted close prices and returns into one DataFrame.\n",
    "    \"\"\"\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for ticker, df in processed_dict.items():\n",
    "        merged_df[f\"{ticker}_Adj_Close\"] = df['Adj Close']\n",
    "        merged_df[f\"{ticker}_Daily_Return\"] = df['Daily_Return']\n",
    "\n",
    "    merged_df.dropna(inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test run: assumes raw data exists in data/raw\n",
    "    tickers = ['SPY', 'TSLA']\n",
    "\n",
    "    # Handle both script and notebook cases\n",
    "    try:\n",
    "        BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        BASE_DIR = os.getcwd()\n",
    "\n",
    "    RAW_DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for t in tickers:\n",
    "        file_path = os.path.join(RAW_DATA_DIR, f\"{t}_raw.csv\")\n",
    "        df = pd.read_csv(file_path ,skiprows=2 ,header=0)  # Skip metadata rows\n",
    "        print(f\"Columns for {t}: {df.columns.tolist()}\")\n",
    "        df.rename(columns={'Price': 'Date'}, inplace=True)\n",
    "        df.rename(columns={'Unnamed: 1': 'Close','Unnamed: 2': 'High','Unnamed: 3': 'Low','Unnamed: 4': 'Open','Unnamed: 5': 'Volume'}, inplace=True)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df.set_index('Date', inplace=True)\n",
    "        raw_data[t] = df\n",
    "\n",
    "    print(raw_data['SPY'].head())\n",
    "    output_dir = 'processed_data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for ticker, df in raw_data.items():\n",
    "        csv_path = os.path.join(output_dir, f'{ticker}_processed.csv')\n",
    "        df.to_csv(csv_path)\n",
    "        print(f'Saved {ticker} data to {csv_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4fa30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
